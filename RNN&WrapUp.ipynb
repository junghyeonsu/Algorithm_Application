{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN&WrapUp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNu62UZvrRD0VGZa8+XBXID",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junghyeonsu/Algorithm_Application/blob/master/RNN%26WrapUp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCmlUROOMY_J",
        "colab_type": "text"
      },
      "source": [
        "# Temporal Multiple Vector Encoding가 중요한 이유 \n",
        "\n",
        "여러개의 데이터를 하나의 데이터로 요약할 수 있다면 어떤 것이 가능할까?\n",
        "\n",
        "여러개로 나누어져있는 데이터를 하나로 요약해\n",
        "하나의 결과를 냄으로써\n",
        "\n",
        "우리가 원하는 정답을 얻거나, 정보를 추출해낼 수 있다.\n",
        "\n",
        "현대에는 자연어처리가 우리 일상생활과 아주 깊은 관련이 있고,\n",
        "\n",
        "대부분의 언어와 관련된 것들은\n",
        "\n",
        "자연어 처리에 사용이 되므로 중요한 컨셉이라고 생각할 수 있다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7KvwxmXNHtD",
        "colab_type": "text"
      },
      "source": [
        "# Temporal Multiple Vector Encoding가 쓰이는 곳\n",
        "\n",
        "1. 언어모델\n",
        "2. 대화모델\n",
        "3. 감성분석모델\n",
        "4. 문서분류모델등등..\n",
        "\n",
        "우리가\n",
        "\n",
        "####알람 켜줘\n",
        "\n",
        "####알람 좀 켜줄래?  \n",
        "\n",
        "####알람 켜줄 수 있어? \n",
        "\n",
        "과 같은 여러가지 토큰으로 나뉜 문장을 하나의 정답으로 내놓을 수가 있다.\n",
        "\n",
        "위애서의 정답은 만약 나온다면 알람 켜기 로 나올 것이다.\n",
        "\n",
        "혹은\n",
        "\n",
        "####이 영화 진짜 재미있었어요\n",
        "\n",
        "####영화 재미있네요\n",
        "\n",
        "####재미있는 영화였어요!\n",
        "\n",
        "와 같은 문장들도 긍정적 이라는 하나의 답으로 내놓을 수 있는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ranVq7qAN94H",
        "colab_type": "text"
      },
      "source": [
        "# RNN\n",
        "\n",
        "Recurrent Neural Network는 N to 1 혹은 N to M 의 작업을 \n",
        "\n",
        "계속해서 반복적으로 과거의 데이터가 현재의 데이터에 영향을 계속주면서\n",
        "\n",
        "가장 마지막 데이터에는 모든 정보가 반영된 데이터가 나오는,\n",
        "\n",
        "반복적으로 반영하는 기법이다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/54893898/83961092-d8a87400-a8ca-11ea-8c60-801b99d3b620.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo0Pfi3jQVC2",
        "colab_type": "text"
      },
      "source": [
        "# RNN에 쓰이는 주요 알고리즘과 패키지들\n",
        "\n",
        "### 알고리즘들\n",
        "\n",
        "RNN의 알고리즘에는 여러가지가 있다. 대표적으로\n",
        "\n",
        "1. Forward RNN\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/54893898/83961188-bbc07080-a8cb-11ea-81c9-7586fec23481.png)\n",
        "\n",
        "2. Backward RNN\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/54893898/83961194-bfec8e00-a8cb-11ea-9918-aaff0470b627.png)\n",
        "\n",
        "3. Bidirectional RNN\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/54893898/83961203-d85ca880-a8cb-11ea-8f8a-bf52f59f7b73.png)\n",
        "\n",
        "4. Stacking RNN\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/54893898/83961207-deeb2000-a8cb-11ea-8100-b8e1e6b3bf3b.png)\n",
        "\n",
        "### 대표적인 패키지\n",
        "\n",
        "RNN을 지원하는 패키지로는\n",
        "\n",
        "1. Keras (케라스)\n",
        "2. Sklearn \n",
        "\n",
        "등등이 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO4BciwTRgbh",
        "colab_type": "text"
      },
      "source": [
        "# RNN의 핵심 아이디어\n",
        "\n",
        "RNN은 여러개의 벡터에서 하나의 벡터로 축약시키는 아이디어가 핵심이다.\n",
        "\n",
        "위에서 말한 Temporal Multiple Vector Encoding을 반복적으로 수행하면서\n",
        "\n",
        "최종 데이터가 나올때까지 반복한다.\n",
        "\n",
        "자연어 처리에 정말 많이쓰이고 대표적이지만,\n",
        "\n",
        "치명적인 단점들이 두가지가 있다.\n",
        "\n",
        "RNN은 전의 데이터값을 다음의 데이터값에 반영시키는 방식으로 진행되는데\n",
        "\n",
        "데이터를 반영한다는 것이 결국은 곱셈이나 덧셈같은 계산들인데\n",
        "\n",
        "뒤로가면 갈수록 앞에 있었던 데이터들이 희석되는 느낌이 있다.\n",
        "\n",
        "그렇기 때문에 멀리 있는 데이터의 정보는 반영되기가 힘든 것이 첫번째이다.\n",
        "\n",
        "두번째는 앞에서 결정을 내릴 때 뒤에있는 데이터에 영향을 더 많이 받을 수 있는데,\n",
        "\n",
        "RNN은 순차적으로 반영하기 때문에 앞에서 계산되는 데이터는\n",
        "\n",
        "뒤에있는 데이터를 반영하기가 불가능하다.\n",
        "\n",
        "이것이 두번째 단점이다.\n",
        "\n",
        "그렇지만 해결방법들은 나와있다.\n",
        "\n",
        "첫번째 멀리있는 데이터의 정보가 반영되기가 힘든 것은\n",
        "\n",
        "Long Short-Term Memory models(LSTM) 이나 \n",
        "\n",
        "GRU(Gated Recurrent Unit) 같은 알고리즘들이 나와있으므로 사용하면 된다. \n",
        "\n",
        "두번째 단점은 Global summarization으로 해결이 가능하다.\n",
        "\n",
        " Global summarization은 Sequence encoding 방식으로\n",
        "\n",
        " 데이터들을 모두 누적시키는 것이다.\n",
        "\n",
        " ![image](https://user-images.githubusercontent.com/54893898/83961352-29b96780-a8cd-11ea-914a-28ee5037f8cb.png)\n",
        "\n",
        " 위 그림과 같이 누적이 된 벡터를 또 다시 모든 토큰들에 대해서 반영을 해주는 것이다.\n",
        "\n",
        " ![image](https://user-images.githubusercontent.com/54893898/83961355-30e07580-a8cd-11ea-93bb-0d27bf6d23b6.png)\n",
        "\n",
        "이렇게 되면 첫번째 토큰인 \"서울역\" 이 \"스타벅스\"에 영향을 많이받는데\n",
        "\n",
        "평범한 RNN 같았으면 \"서울역\"이 제일 먼저 계산이 되므로\n",
        "\n",
        "뒤에있는 \"스타벅스\"에 영향을 받은 데이터는 계산이 안된다.\n",
        "\n",
        "그렇지만 Sequence encoding 방식으로 처리속도는 조금 느려질지라도\n",
        "\n",
        "모든 데이터의 요약이 들어가있는 Hidden Variable을 모든 토큰에 반영함으로써\n",
        "\n",
        "공정하게 점수를 매기는 것이다.\n",
        "\n",
        "그렇지만 RNN은 문장이 길어지면 이런 좋은 알고리즘들을 써도 번역이 잘 안되는 문제점이 있다.\n",
        "\n",
        "출처 : 충남대학교 컴퓨터공학과 정상근 교수님 알고리즘응용수업 ppt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4V60E-hUnsn",
        "colab_type": "text"
      },
      "source": [
        "# WRAP UP (Project 2)\n",
        "\n",
        "우리가 지금까지 배웠던 알고리즘들중에서\n",
        "\n",
        "* Classification\n",
        "\n",
        "* Neural Network\n",
        "\n",
        "* RNN\n",
        "\n",
        "* Viterbi Search\n",
        "\n",
        "를 이용한 pos tagger 를 만드는 과정을 간단히 소개한다.\n",
        "\n",
        "\n",
        "# pos tagger\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/54893898/83961464-3a1e1200-a8ce-11ea-9ead-f06a2d8bb21f.png)\n",
        "\n",
        "\n",
        "part of speech tagger로써 문장이 주어지면\n",
        "\n",
        "각각의 단어들을 토큰으로 취급하고\n",
        "\n",
        "각각의 토큰들의 품사를 판별해 태그를 붙여주는 테크닉이다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwjAXk8XYm6Y",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/54893898/83961663-528f2c00-a8d0-11ea-9e78-a49d418fc150.png)\n",
        "\n",
        "위 그림에서 토큰들이 각각 단어들의 인덱스로 바뀐다.\n",
        "\n",
        "그 다음 단어들을 벡터로 표시해주는 기술인 Embedding을 통해서 나온 것을\n",
        "\n",
        "또 우리가 만들어준 DNN에 List of referece class index와 함께 계산해준다.\n",
        "\n",
        "List of referece class index는 우리가 정답값을 아예 대입을 해준 배열이라고 보면된다.\n",
        "\n",
        "만약 맨 첫번째 인덱스에 명사가 왔다면 \n",
        "\n",
        "인덱스 첫번째의 값인 3은 명사라고 생각하면 되는 것이다.\n",
        "\n",
        "그럼 Number of target class가 나오는데 \n",
        "\n",
        "그림의 경우에는 8개의 토큰에 대해서 각각 행이\n",
        "\n",
        "4개의 태그에 대응이 된다는 뜻이다.\n",
        "\n",
        "0,1,2,3 인덱스가 있는데 만약 명사가 2라면,\n",
        "\n",
        "Number of target class의 첫번째 행의 인덱스는\n",
        "\n",
        "[0, 0, 1, 0] 이런식으로 배치가 될 것이다.\n",
        "\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/54893898/83961798-b108da00-a8d1-11ea-90b6-b910cfdf9ff8.png)\n",
        "\n",
        "Neural network 구조를 자세히 들여다보면\n",
        "\n",
        "X 값인 토큰값들이 들어오고 Embedding 작업,\n",
        "\n",
        "Dropout은 테스팅환경에서 성능이 좋아지는 작업이다.\n",
        "\n",
        "그 다음 RNN을 거치면 문장 전체가 encoding이 될것이다.\n",
        "\n",
        "그러고 예측값과 정답값을 cross entrophy를 해준다.\n",
        "\n",
        "그리고 나오는 Loss를 계속해서 고치는 방향으로 \n",
        "\n",
        "Neural network이 설계된다고 보면된다.\n",
        "\n",
        "pos tagging 작업은 이런식으로 동작한다고 보면된다."
      ]
    }
  ]
}